{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-27T12:51:48.498002Z","iopub.execute_input":"2022-11-27T12:51:48.498652Z","iopub.status.idle":"2022-11-27T12:52:18.207069Z","shell.execute_reply.started":"2022-11-27T12:51:48.498532Z","shell.execute_reply":"2022-11-27T12:52:18.206105Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing import image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n%matplotlib inline\nimport os\nimport torch\nfrom torch.utils.data import Dataset\nfrom skimage import io\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision\nfrom torch.utils.data import DataLoader\n# from torchsummary import summary\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:52:32.405385Z","iopub.execute_input":"2022-11-27T12:52:32.405813Z","iopub.status.idle":"2022-11-27T12:52:46.782269Z","shell.execute_reply.started":"2022-11-27T12:52:32.405780Z","shell.execute_reply":"2022-11-27T12:52:46.781207Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary\nfrom torchsummary import summary","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:45:58.277082Z","iopub.execute_input":"2022-11-27T10:45:58.277823Z","iopub.status.idle":"2022-11-27T10:46:11.354384Z","shell.execute_reply.started":"2022-11-27T10:45:58.277778Z","shell.execute_reply":"2022-11-27T10:46:11.352643Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"X_train = pd.read_csv('/kaggle/input/col774-2022/train_x.csv')    # reading the csv file\nX_train.head() ","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:02.704553Z","iopub.execute_input":"2022-11-27T12:53:02.705560Z","iopub.status.idle":"2022-11-27T12:53:02.850708Z","shell.execute_reply.started":"2022-11-27T12:53:02.705525Z","shell.execute_reply":"2022-11-27T12:53:02.849510Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Id Cover_image_name                                              Title\n0   0        33735.jpg  Lutheran and Catholic Reconciliation on Justif...\n1   1        36310.jpg  Uncertain Justice: The Roberts Court : and the...\n2   2        23735.jpg                     Currency ; Trading For Dummies\n3   3        45580.jpg  : Twins, Triplets, and More: Their Nature, Dev...\n4   4        14290.jpg  The 2013 ; Import and Export Market for Printe...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Cover_image_name</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>33735.jpg</td>\n      <td>Lutheran and Catholic Reconciliation on Justif...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>36310.jpg</td>\n      <td>Uncertain Justice: The Roberts Court : and the...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>23735.jpg</td>\n      <td>Currency ; Trading For Dummies</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>45580.jpg</td>\n      <td>: Twins, Triplets, and More: Their Nature, Dev...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>14290.jpg</td>\n      <td>The 2013 ; Import and Export Market for Printe...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_train= pd.read_csv(\"/kaggle/input/col774-2022/train_y.csv\")\ny_test= pd.read_csv(\"/kaggle/input/col774-2022/non_comp_test_y.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:06.728983Z","iopub.execute_input":"2022-11-27T12:53:06.729362Z","iopub.status.idle":"2022-11-27T12:53:06.762984Z","shell.execute_reply.started":"2022-11-27T12:53:06.729332Z","shell.execute_reply":"2022-11-27T12:53:06.762117Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"X_test = pd.read_csv(\"/kaggle/input/col774-2022/non_comp_test_x.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:09.998998Z","iopub.execute_input":"2022-11-27T12:53:09.999442Z","iopub.status.idle":"2022-11-27T12:53:10.034948Z","shell.execute_reply.started":"2022-11-27T12:53:09.999403Z","shell.execute_reply":"2022-11-27T12:53:10.033943Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"y_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:46:11.672525Z","iopub.execute_input":"2022-11-27T10:46:11.674841Z","iopub.status.idle":"2022-11-27T10:46:11.688285Z","shell.execute_reply.started":"2022-11-27T10:46:11.674798Z","shell.execute_reply":"2022-11-27T10:46:11.687114Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Id  Genre\n0   0      4\n1   1     16\n2   2     23\n3   3      7\n4   4     28","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_test.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:46:11.689700Z","iopub.execute_input":"2022-11-27T10:46:11.690162Z","iopub.status.idle":"2022-11-27T10:46:11.712138Z","shell.execute_reply.started":"2022-11-27T10:46:11.690118Z","shell.execute_reply":"2022-11-27T10:46:11.711040Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   Id Cover_image_name                                              Title\n0   0        23538.jpg  Venus and , Serena Williams (Great African Ame...\n1   1        11016.jpg  Faithfully Yours: The : Amazing Bond Between U...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Cover_image_name</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>23538.jpg</td>\n      <td>Venus and , Serena Williams (Great African Ame...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11016.jpg</td>\n      <td>Faithfully Yours: The : Amazing Bond Between U...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.merge(X_train, y_train, on='Id')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:17.155504Z","iopub.execute_input":"2022-11-27T12:53:17.156475Z","iopub.status.idle":"2022-11-27T12:53:17.196188Z","shell.execute_reply.started":"2022-11-27T12:53:17.156419Z","shell.execute_reply":"2022-11-27T12:53:17.195205Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:46:11.770689Z","iopub.execute_input":"2022-11-27T10:46:11.773066Z","iopub.status.idle":"2022-11-27T10:46:11.788020Z","shell.execute_reply.started":"2022-11-27T10:46:11.773025Z","shell.execute_reply":"2022-11-27T10:46:11.787155Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   Id Cover_image_name                                              Title  \\\n0   0        33735.jpg  Lutheran and Catholic Reconciliation on Justif...   \n1   1        36310.jpg  Uncertain Justice: The Roberts Court : and the...   \n2   2        23735.jpg                     Currency ; Trading For Dummies   \n3   3        45580.jpg  : Twins, Triplets, and More: Their Nature, Dev...   \n4   4        14290.jpg  The 2013 ; Import and Export Market for Printe...   \n\n   Genre  \n0      4  \n1     16  \n2     23  \n3      7  \n4     28  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Cover_image_name</th>\n      <th>Title</th>\n      <th>Genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>33735.jpg</td>\n      <td>Lutheran and Catholic Reconciliation on Justif...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>36310.jpg</td>\n      <td>Uncertain Justice: The Roberts Court : and the...</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>23735.jpg</td>\n      <td>Currency ; Trading For Dummies</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>45580.jpg</td>\n      <td>: Twins, Triplets, and More: Their Nature, Dev...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>14290.jpg</td>\n      <td>The 2013 ; Import and Export Market for Printe...</td>\n      <td>28</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test= pd.merge(X_test, y_test, on='Id')\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:21.248843Z","iopub.execute_input":"2022-11-27T12:53:21.249241Z","iopub.status.idle":"2022-11-27T12:53:21.268039Z","shell.execute_reply.started":"2022-11-27T12:53:21.249208Z","shell.execute_reply":"2022-11-27T12:53:21.266932Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Id Cover_image_name                                              Title  \\\n0   0        23538.jpg  Venus and , Serena Williams (Great African Ame...   \n1   1        11016.jpg  Faithfully Yours: The : Amazing Bond Between U...   \n2   2          485.jpg  The China Mirage: The : Hidden History of Amer...   \n3   3        21793.jpg  My Kind of Sad: What ; It's Like to Be Young a...   \n4   4        23195.jpg  ASE Test Preparation - T1 Gasoline Engines (As...   \n\n   Genre  \n0     25  \n1      1  \n2     13  \n3     21  \n4      6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Cover_image_name</th>\n      <th>Title</th>\n      <th>Genre</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>23538.jpg</td>\n      <td>Venus and , Serena Williams (Great African Ame...</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>11016.jpg</td>\n      <td>Faithfully Yours: The : Amazing Bond Between U...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>485.jpg</td>\n      <td>The China Mirage: The : Hidden History of Amer...</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>21793.jpg</td>\n      <td>My Kind of Sad: What ; It's Like to Be Young a...</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>23195.jpg</td>\n      <td>ASE Test Preparation - T1 Gasoline Engines (As...</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class genre_class(Dataset):\n    def __init__(self, dataframe, root_dir, transform= None):\n        self.annotations= dataframe\n        self.root_dir= root_dir\n        self.transform= transform\n        \n    def __len__(self):\n        return len(self.annotations)\n    \n    def __getitem__(self, index):\n        img_path= os.path.join(self.root_dir, self.annotations.iloc[index,1])\n#         image= io.imread(img_path)\n        image = Image.open(img_path).convert('RGB')\n        y_label= torch.tensor(int(self.annotations.iloc[index,3]))\n        \n        if self.transform:\n            image= self.transform(image)\n            \n        return(image, y_label)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:26.709344Z","iopub.execute_input":"2022-11-27T12:53:26.709777Z","iopub.status.idle":"2022-11-27T12:53:26.720158Z","shell.execute_reply.started":"2022-11-27T12:53:26.709743Z","shell.execute_reply":"2022-11-27T12:53:26.719012Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Hyper parameters","metadata":{}},{"cell_type":"code","source":"in_channel= 3\nnum_classes= 30\nlearning_rate= 0.001\nbatch_size = 64\nnum_epochs= 20\n# Device will determine whether to run the training on GPU or CPU.\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:32.834681Z","iopub.execute_input":"2022-11-27T12:53:32.835347Z","iopub.status.idle":"2022-11-27T12:53:32.953129Z","shell.execute_reply.started":"2022-11-27T12:53:32.835311Z","shell.execute_reply":"2022-11-27T12:53:32.952019Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Load data","metadata":{}},{"cell_type":"code","source":"train_set= genre_class(dataframe= train, root_dir='/kaggle/input/col774-2022/images/images/', \n              transform= transforms.Compose([\n                  transforms.Resize((128), antialias=True),\n                  transforms.ToTensor()]\n              ))\ntrain_loader= DataLoader(dataset=train_set,batch_size= batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:37.389661Z","iopub.execute_input":"2022-11-27T12:53:37.390018Z","iopub.status.idle":"2022-11-27T12:53:37.396249Z","shell.execute_reply.started":"2022-11-27T12:53:37.389989Z","shell.execute_reply":"2022-11-27T12:53:37.395042Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"test= genre_class(dataframe= test, root_dir='/kaggle/input/col774-2022/images/images/', \n              transform= transforms.Compose([\n                  transforms.Resize((128), antialias=True),\n                  transforms.ToTensor()]\n              ))\ntest_loader= DataLoader(dataset=test,batch_size= batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:53:42.389416Z","iopub.execute_input":"2022-11-27T12:53:42.389857Z","iopub.status.idle":"2022-11-27T12:53:42.401559Z","shell.execute_reply.started":"2022-11-27T12:53:42.389819Z","shell.execute_reply":"2022-11-27T12:53:42.400386Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for ele in train_loader:\n    print(ele[0].shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:46:12.033900Z","iopub.execute_input":"2022-11-27T10:46:12.036420Z","iopub.status.idle":"2022-11-27T10:46:12.605313Z","shell.execute_reply.started":"2022-11-27T10:46:12.036377Z","shell.execute_reply":"2022-11-27T10:46:12.604182Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 128, 128])\n","output_type":"stream"}]},{"cell_type":"code","source":"for ele in test_loader:\n    print(ele[0].shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:54:07.784383Z","iopub.execute_input":"2022-11-27T12:54:07.784820Z","iopub.status.idle":"2022-11-27T12:54:08.358301Z","shell.execute_reply.started":"2022-11-27T12:54:07.784786Z","shell.execute_reply":"2022-11-27T12:54:08.357162Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"torch.Size([64, 3, 128, 128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"create model","metadata":{}},{"cell_type":"code","source":"# Creating a CNN class\nclass ConvNeuralNet(nn.Module):\n# Determine what layers and their order in CNN object \n    def __init__(self, num_classes):\n        super(ConvNeuralNet, self).__init__()\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5)\n        self.relu1 = nn.ReLU()\n        self.max_pool1 = nn.MaxPool2d(kernel_size = 2)\n#         print(self.max_pool1.size())\n        # Current Image Size = (62 X 62 X 32)\n        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n        self.relu2 = nn.ReLU()\n        self.max_pool2 = nn.MaxPool2d(kernel_size = 2)\n        # Current Image Size = (29 X 29 X 64)\n        self.conv_layer3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5)\n        self.relu3 = nn.ReLU()\n        self.max_pool3 = nn.MaxPool2d(kernel_size = 2)\n#         Curremt Image Size = (12 X 12 X 128)\n        self.fc1 = nn.Linear(18432, 128)\n#         self.fc1 = nn.Linear(144,128)\n        self.relu4 = nn.ReLU()\n        self.fc2 = nn.Linear(128, num_classes)\n        \n       # Progresses data across layers    \n    def forward(self, x):\n#         print(x.size())\n        out = self.conv_layer1(x)\n#         print(out.size())\n        out = self.relu1(out)\n#         print(out.size())\n        out = self.max_pool1(out)\n#         print(out.size())\n        out = self.conv_layer2(out)\n#         print(out.size())\n        out = self.relu2(out)\n#         print(out.size())\n        out = self.max_pool2(out)\n#         print(out.size())\n        out = self.conv_layer3(out)\n#         print(out.size())\n        out = self.relu3(out)\n#         print(out.size())\n        out = self.max_pool3(out)    \n#         print(out.size())\n        out = out.reshape(out.size(0), -1)\n#         print(out.size())\n#         out = torch.flatten(out)\n#         print(out.size())\n        out = self.fc1(out)\n#         print(out.size())\n        out = self.relu4(out)\n#         print(out.size())\n        out = self.fc2(out)\n#         print(out.size())\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:54:15.222795Z","iopub.execute_input":"2022-11-27T12:54:15.223522Z","iopub.status.idle":"2022-11-27T12:54:15.243842Z","shell.execute_reply.started":"2022-11-27T12:54:15.223477Z","shell.execute_reply":"2022-11-27T12:54:15.242563Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = ConvNeuralNet(num_classes)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:54:20.543719Z","iopub.execute_input":"2022-11-27T12:54:20.544069Z","iopub.status.idle":"2022-11-27T12:54:20.577660Z","shell.execute_reply.started":"2022-11-27T12:54:20.544041Z","shell.execute_reply":"2022-11-27T12:54:20.576821Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Set Loss function with criterion\ncriterion = nn.CrossEntropyLoss()\n\n# Set optimizer with optimizer\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)  \n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:54:23.953667Z","iopub.execute_input":"2022-11-27T12:54:23.954903Z","iopub.status.idle":"2022-11-27T12:54:23.961230Z","shell.execute_reply.started":"2022-11-27T12:54:23.954819Z","shell.execute_reply":"2022-11-27T12:54:23.960306Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"train network ","metadata":{}},{"cell_type":"code","source":"# We use the pre-defined number of epochs to determine how many iterations to train the network on\nfor epoch in range(num_epochs):\n#Load in the data in batches using the train_loader object\n    for i, (images, labels) in enumerate(train_loader): \n#         break\n#     break\n        # Forward pass\n        try:\n            images = images.to(device)\n            labels = labels.to(device)\n#             model= nn.DataParallel(model)\n            model = model.to(device)\n#             model = model.to(device)\n            outputs = model(images)\n    #         print(outputs)\n            labels = labels.to(torch.long)\n            outputs = outputs.to(torch.double)\n    #         print(labels)\n            outputs  = torch.reshape(outputs, (batch_size,-1))\n    #         print(outputs)\n            loss = criterion(outputs, labels)\n#             print(i, loss.item())\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        except Exception as e:\n            print(e) \n            continue\n    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:49:07.426319Z","iopub.execute_input":"2022-11-27T10:49:07.427304Z","iopub.status.idle":"2022-11-27T11:30:59.706759Z","shell.execute_reply.started":"2022-11-27T10:49:07.427263Z","shell.execute_reply":"2022-11-27T11:30:59.705695Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"shape '[64, -1]' is invalid for input of size 720\nEpoch [1/20], Loss: 3.3033\nshape '[64, -1]' is invalid for input of size 720\nEpoch [2/20], Loss: 3.2935\nshape '[64, -1]' is invalid for input of size 720\nEpoch [3/20], Loss: 3.2713\nshape '[64, -1]' is invalid for input of size 720\nEpoch [4/20], Loss: 3.0921\nshape '[64, -1]' is invalid for input of size 720\nEpoch [5/20], Loss: 2.7443\nshape '[64, -1]' is invalid for input of size 720\nEpoch [6/20], Loss: 2.5028\nshape '[64, -1]' is invalid for input of size 720\nEpoch [7/20], Loss: 2.1902\nshape '[64, -1]' is invalid for input of size 720\nEpoch [8/20], Loss: 1.7527\nshape '[64, -1]' is invalid for input of size 720\nEpoch [9/20], Loss: 1.9730\nshape '[64, -1]' is invalid for input of size 720\nEpoch [10/20], Loss: 1.3831\nshape '[64, -1]' is invalid for input of size 720\nEpoch [11/20], Loss: 1.5040\nshape '[64, -1]' is invalid for input of size 720\nEpoch [12/20], Loss: 1.1769\nshape '[64, -1]' is invalid for input of size 720\nEpoch [13/20], Loss: 1.2692\nshape '[64, -1]' is invalid for input of size 720\nEpoch [14/20], Loss: 1.1985\nshape '[64, -1]' is invalid for input of size 720\nEpoch [15/20], Loss: 1.0166\nshape '[64, -1]' is invalid for input of size 720\nEpoch [16/20], Loss: 0.8778\nshape '[64, -1]' is invalid for input of size 720\nEpoch [17/20], Loss: 0.6176\nshape '[64, -1]' is invalid for input of size 720\nEpoch [18/20], Loss: 0.6668\nshape '[64, -1]' is invalid for input of size 720\nEpoch [19/20], Loss: 0.4601\nshape '[64, -1]' is invalid for input of size 720\nEpoch [20/20], Loss: 0.5465\n","output_type":"stream"}]},{"cell_type":"code","source":"def check_accuracy(loader, model):\n    num_correct= 0\n    num_samples= 0\n    model.eval()\n    \n    with torch.no_grad():\n        for x,y in loader:\n            score= model(x)\n            _, predictions= score.max(1)\n            num_correct += (predictions==y.sum())\n            num_smaples += predictions.size()\n    \n    model.train()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T11:31:07.572043Z","iopub.execute_input":"2022-11-27T11:31:07.572487Z","iopub.status.idle":"2022-11-27T11:31:07.579492Z","shell.execute_reply.started":"2022-11-27T11:31:07.572450Z","shell.execute_reply":"2022-11-27T11:31:07.578395Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    print('Accuracy of the network on the {} train images: {} %'.format(50000, 100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T11:32:02.865538Z","iopub.execute_input":"2022-11-27T11:32:02.865947Z","iopub.status.idle":"2022-11-27T11:34:37.961324Z","shell.execute_reply.started":"2022-11-27T11:32:02.865912Z","shell.execute_reply":"2022-11-27T11:34:37.960111Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Accuracy of the network on the 50000 train images: 43.66374269005848 %\n","output_type":"stream"}]},{"cell_type":"code","source":"# We use the pre-defined number of epochs to determine how many iterations to train the network on\nfor epoch in range(num_epochs):\n#Load in the data in batches using the train_loader object\n    for i, (images, labels) in enumerate(test_loader): \n#         break\n#     break\n        # Forward pass\n        try:\n            images = images.to(device)\n            labels = labels.to(device)\n#             model= nn.DataParallel(model)\n            model = model.to(device)\n#             model = model.to(device)\n            outputs = model(images)\n    #         print(outputs)\n            labels = labels.to(torch.long)\n            outputs = outputs.to(torch.double)\n    #         print(labels)\n            outputs  = torch.reshape(outputs, (batch_size,-1))\n    #         print(outputs)\n            loss = criterion(outputs, labels)\n#             print(i, loss.item())\n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        except Exception as e:\n            print(e) \n            continue\n    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:54:46.080580Z","iopub.execute_input":"2022-11-27T12:54:46.081020Z","iopub.status.idle":"2022-11-27T13:02:01.759868Z","shell.execute_reply.started":"2022-11-27T12:54:46.080986Z","shell.execute_reply":"2022-11-27T13:02:01.758693Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"shape '[64, -1]' is invalid for input of size 120\nEpoch [1/20], Loss: 3.4026\nshape '[64, -1]' is invalid for input of size 120\nEpoch [2/20], Loss: 3.3783\nshape '[64, -1]' is invalid for input of size 120\nEpoch [3/20], Loss: 3.3694\nshape '[64, -1]' is invalid for input of size 120\nEpoch [4/20], Loss: 3.3759\nshape '[64, -1]' is invalid for input of size 120\nEpoch [5/20], Loss: 3.3086\nshape '[64, -1]' is invalid for input of size 120\nEpoch [6/20], Loss: 3.1484\nshape '[64, -1]' is invalid for input of size 120\nEpoch [7/20], Loss: 3.1127\nshape '[64, -1]' is invalid for input of size 120\nEpoch [8/20], Loss: 2.8552\nshape '[64, -1]' is invalid for input of size 120\nEpoch [9/20], Loss: 2.4773\nshape '[64, -1]' is invalid for input of size 120\nEpoch [10/20], Loss: 2.2447\nshape '[64, -1]' is invalid for input of size 120\nEpoch [11/20], Loss: 1.7267\nshape '[64, -1]' is invalid for input of size 120\nEpoch [12/20], Loss: 1.4585\nshape '[64, -1]' is invalid for input of size 120\nEpoch [13/20], Loss: 1.2710\nshape '[64, -1]' is invalid for input of size 120\nEpoch [14/20], Loss: 1.3292\nshape '[64, -1]' is invalid for input of size 120\nEpoch [15/20], Loss: 1.1477\nshape '[64, -1]' is invalid for input of size 120\nEpoch [16/20], Loss: 0.8107\nshape '[64, -1]' is invalid for input of size 120\nEpoch [17/20], Loss: 0.7098\nshape '[64, -1]' is invalid for input of size 120\nEpoch [18/20], Loss: 0.8204\nshape '[64, -1]' is invalid for input of size 120\nEpoch [19/20], Loss: 0.5415\nshape '[64, -1]' is invalid for input of size 120\nEpoch [20/20], Loss: 0.4851\n","output_type":"stream"}]},{"cell_type":"code","source":"with torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    print('Accuracy of the network on the test images: {} %'.format( 100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2022-11-27T13:03:35.238604Z","iopub.execute_input":"2022-11-27T13:03:35.239387Z","iopub.status.idle":"2022-11-27T13:03:56.884010Z","shell.execute_reply.started":"2022-11-27T13:03:35.239352Z","shell.execute_reply":"2022-11-27T13:03:56.882924Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy of the network on the test images: 75.56140350877193 %\n","output_type":"stream"}]}]}